{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce challenge est de proposer une méthode de classification, basée sur des réseaux de neurones, permettant de classer des images issues de Google Quickdraw (https://quickdraw.withgoogle.com/data).\n",
    "Le jeu de données proposé comprend 5 classes balancées, avec 15000 exemples d'apprentissage et 5000 exemples de validation : des paniers, des yeux, des lunettes, des lapins et des mains.\n",
    "\n",
    "Vous pouvez utiliser tout algorithme qui vous semble pertinent (PMC, CNN,), en faisant varier les données d'entrée (normalisation, augmentation de données,...), les paramètres des réseaux considérés (fonction objectif, optimiseur, dropout, learning rate, taille des batchs...)\n",
    "\n",
    "Bien sûr, pour valider votre travail, nous ferons tourner le code en local, sur la base originale que nous conservons, pour voir si les résultats que vous proposez sont reproductibles. \n",
    "\n",
    "Le compte-rendu sera effectué sur un notebook jupyter, dans lequel vous reporterez votre méthodologie en markdown, et vos codes en Python. Vous expliquerez votre démarche, justifierez vos choix, commenterez vos expérimentations et vos résultats.\n",
    "\n",
    "La notation sera construite de la manière suivante : \n",
    "- 12 points sur le compte-rendu\n",
    "- 8 points seront accordés sur un classement par le taux de reconnsaissance des objets de la base de test. Les binômes seront classés par taux de reconnaissance décroissant, les premiers ayant 8 points accordés, les autres binômes ayant une note suivant une décroissance linéaire.\n",
    "\n",
    "Le notebook sera déposé sur la plateforme moodle du cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print(tensorflow.version.VERSION)\n",
    "# Ajouter les imports nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données pour l'entraînement\n",
    "\n",
    "On utilise le module `pandas`, il facilite grandement le chargement de fichier CSV, en les lisant sous-forme de matrices appelées **Dataframes**.\n",
    "\n",
    "L'utilisation du paramètre `dtype` est obligatoire pour indiquer à `pandas` de charger le contenu des cases sous la forme de chaînes de caractères. Cela évite une transformation pour obtenir les étiquettes des classes qui doit obligatoirement être de ce type là."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image_name             relative_path class_label\n",
      "0      basket_052681.png  basket/basket_052681.png           0\n",
      "1      basket_028248.png  basket/basket_028248.png           0\n",
      "2      basket_050738.png  basket/basket_050738.png           0\n",
      "3      basket_034414.png  basket/basket_034414.png           0\n",
      "4      basket_091738.png  basket/basket_091738.png           0\n",
      "...                  ...                       ...         ...\n",
      "74995    hand_241467.png      hand/hand_241467.png           4\n",
      "74996    hand_052619.png      hand/hand_052619.png           4\n",
      "74997    hand_168727.png      hand/hand_168727.png           4\n",
      "74998    hand_285523.png      hand/hand_285523.png           4\n",
      "74999    hand_263562.png      hand/hand_263562.png           4\n",
      "\n",
      "[75000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"train.csv\", dtype=\"str\")\n",
    "validData = pd.read_csv('valid.csv', dtype=\"str\")\n",
    "print(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de données\n",
    "\n",
    "Nous utilisons l'outil de génération de données `ImageDataGenerator`pour faciliter la manipulation du Dataframe.\n",
    "Comme les données de validation sont stockées dans un second fichier, il n'est pas nécessaire d'utiliser un `validation_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImageGenerator = ImageDataGenerator(\n",
    "    validation_split=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle\n",
    "\n",
    "Selu est légèrement meilleur que Relu dans notre cas. Sigmoid ne fonctionne pas du tout.\n",
    "### InputLayer\n",
    "Sa dimension correspond à la résolution des images utilisées.\n",
    "\n",
    "### Reshape\n",
    "On modifie la dimension pour convertir toutes les images qui étaient en couleur en niveau de gris.\n",
    "\n",
    "### Conv2D\n",
    "On réduit la taille du layer en moyennant les pixels d'un carré 3x3.\n",
    "\n",
    "### MaxPooling2D\n",
    "Sous-échantillonage en ne conservant que la valeur maximum des sous-matrices (2x2).  Une taille de sous-matrice plus importante entraienerait une trop grosse perte d'informations.\n",
    "\n",
    "### Dropout\n",
    "Cette couche permet d'introduire de l'aléatoire dans le modèle pour le faire évoluer. On ignore certains neurones pour voir introduire des variations de résultats. Ici *25%* des neurones sont abandonnés. \n",
    "\n",
    "## Flatten\n",
    "Permet de changer la dimension pour atteindre une dimension.\n",
    "\n",
    "### Dense\n",
    "Sa dimension est (5), car notre modèle détermine une classe parmis 5 disponibles.\n",
    "On utilise la fonction `softmax` car nous souhaitons sélectionner une seule classe parmis celles disponnibles. On séléctionne celle avec la plus grande probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(28, 28)), # Résolution des images\n",
    "        keras.layers.Reshape((28, 28, 1)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='selu'),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='selu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation du modèle\n",
    "\n",
    "### Optimizer\n",
    "Pour séléctionner la fonction d'optimisation, nous avons décidé de bloquer les autres paramètres de la simulation, et de conserver la fonction qui obtient les meilleurs résultats en 10 epochs.\n",
    "\n",
    "| Fonction | Validation accuracy (approx.) |\n",
    "| --- | --- |\n",
    "| SGD | 92% |\n",
    "| Adam | 92% |\n",
    "| Adagrad | 93% |\n",
    "| Ftrl | 89% |\n",
    "| RMSprop | 91% |\n",
    "| Adadelta | 61% |\n",
    "| Nadam | 92% |\n",
    "\n",
    "Nous avons donc décidé de converver le `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 46085     \n",
      "=================================================================\n",
      "Total params: 64,901\n",
      "Trainable params: 64,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 non-validated image filenames belonging to 5 classes.\n",
      "Found 25000 non-validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Images utilisées pour l'entrainement\n",
    "train_images = trainImageGenerator.flow_from_dataframe(\n",
    "    dataframe=trainData, # Données provenant de pandas pour l'entrainement\n",
    "    directory=\"images\", # Dossier racine des images\n",
    "    x_col=\"relative_path\", # Chemin d'acces des images\n",
    "    y_col=\"class_label\", # Nom des classes \n",
    "    class_mode=\"categorical\", # Type de classification : \n",
    "    target_size=(28,28),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=750,\n",
    "    validate_filenames=False, # Pas de vérification des fichiers pour gagner du temps\n",
    "    shuffle=True # En rendant aléatoire les données on augmente les chances d'avoir un lancement meilleur\n",
    ")\n",
    "\n",
    "# Images utilisées pour la validation\n",
    "validation_images = trainImageGenerator.flow_from_dataframe(\n",
    "    dataframe=validData, # Données provenant de pandas pour la validation\n",
    "    directory=\"images\",\n",
    "    x_col=\"relative_path\",\n",
    "    y_col=\"class_label\",\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(28,28),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=750,\n",
    "    validate_filenames=False # Pas de vérification des fichiers pour gagner du temps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle\n",
    "\n",
    "Pour entraîner le modèle, nous utilisons les images provenant du fichier d'entrainement. Et nous utilisons celle de l'autre fichier pour faire la validation de la progression.\n",
    "\n",
    "Nous avons choisis de faire seulement 5 epochs, au-delà la progression ne semble pas très élevée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"input_1:0\", shape=(None, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"input_1:0\", shape=(None, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.5793 - categorical_accuracy: 0.7210WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"input_1:0\", shape=(None, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 4.5793 - categorical_accuracy: 0.7210 - val_loss: 0.5072 - val_categorical_accuracy: 0.8616\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, validation_data=validation_images, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,3))\n",
    "fig.suptitle(\"Résultats de l'apprentissage\")\n",
    "ax1.plot(history.history['categorical_accuracy'])\n",
    "ax1.plot(history.history['val_categorical_accuracy'])\n",
    "ax1.legend(['categorical_accuracy', 'val_categorical_accuracy'])\n",
    "\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.legend(['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de classification par le modèle\n",
    "\n",
    "Pour afficher quelques exemples nous allons réutiliser les données utilisées lors de la validation. On demande au modèle de prédire la classe des images, puis on extrait une partie de ces résultats. Enfin on affiche le résultat après l'avoir convertie en texte plus explicite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"basket\", \"eye\", \"binoculars\", \"rabbit\", \"hand\"]\n",
    "\n",
    "def load(filename):\n",
    "    np_image = Image.open(filename)\n",
    "    np_image = image.img_to_array(np_image)\n",
    "    np_image = np.expand_dims(np_image, axis = 0)\n",
    "    return np_image\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "for i in range(0,5):\n",
    "    ax= plt.subplot(1, 5 ,i+1)\n",
    "    im = Image.open('images/'+classes[i]+'/'+ os.listdir(\"images/\"+classes[i])[0])\n",
    "    fig=ax.imshow(im)\n",
    "    prediction = model.predict(load('images/'+classes[i]+'/'+ os.listdir(\"images/\"+classes[i])[0]))\n",
    "    prediction = np.argmax(prediction, axis = 1)[0]\n",
    "    plt.title(classes[prediction])\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle\n",
    "\n",
    "On utilise le format **HDF5** pour stocker le modèle, ce format est pratique car il permet de stocker l'architecture du modèle, les poids ainsi que les informations de compilation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pouly_zangla_classifier.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
